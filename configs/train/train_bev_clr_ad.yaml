# ======================
# Model / experiment name
# ======================
exp_name: 'BEV_CLR_AD'           # Used to build run names, checkpoint/log folders, and the W&B project name.

# ======================
# Training parameters
# ======================
max_iters: 50000                 # Total optimization steps (NOT epochs). Effective data seen depends on dataloader size.
log_freq: 100                    # How often to log scalars/images to TensorBoard/W&B (in steps).
shuffle: true                    # Shuffle training samples each epoch (affects dataloader).
dset: 'trainval'                 # Dataset split to load in nuscenes_data.compile_data (e.g., 'train', 'trainval', 'mini').
save_freq: 1000                  # Save a checkpoint every N steps.
batch_size: 1                    # Per-step mini-batch size (must be divisible by len(device_ids) if using DataParallel).
grad_acc: 5                      # Gradient accumulation steps. Effective batch size = batch_size * grad_acc.
lr: 0.0003                       # Peak LR for OneCycleLR (if use_scheduler=true); base LR for constant schedule otherwise.
use_scheduler: true              # Use OneCycleLR schedule (warms up then anneals).
weight_decay: 0.0000001          # Adam/AdamW weight decay (L2 regularization).
nworkers: 16                     # DataLoader workers (increase until I/O saturates but avoid fork issues on your system).

# ======================
# Directories
# ======================
data_dir: '/../../../../../../beegfs/scratch/workspace/es_luheit04-NuScneDataset_new/es_luheit04-NuSceneDataset-1756172124/nuscenes'
                                 # Root of nuScenes (must contain 'samples', 'sweeps', 'maps', etc.).
custom_dataroot: 'datasets/scaled_images' 
                                 # Optional pre-scaled images root used by the loader when use_pre_scaled_imgs=true.
log_dir: 'logs_nuscenes'         # TensorBoard log root (a subfolder per run is created automatically).
ckpt_dir: 'model_checkpoints'    # Where checkpoints are saved (a subfolder per run is created).
keep_latest: 75                  # Keep only the latest N checkpoints (older ones are pruned).
init_dir: ''                     # Path to a checkpoint to resume from (folder containing ckpt + meta).
ignore_load: null                # Optional list/pattern of keys to ignore when loading a checkpoint state_dict.
load_step: true                  # When resuming, load global_step from the checkpoint (controls naming/logging).
load_optimizer: true             # When resuming, also load optimizer state.
load_scheduler: true             # When resuming, also load scheduler state.

# ======================
# Data parameters
# ======================
final_dim: [448, 896]            # (H, W) image size after aug; chosen to be friendly to ViT/DINO downsample ratios.
rand_flip: false                 # Random horizontal/vertical flips for images/BEV (the model “unflips” internally).
rand_crop_and_resize: false      # If true, use random resize + crop in the dataloader (resize_lim controls range).
ncams: 6                         # Number of cameras per sample (nuScenes uses 6).
nsweeps: 5                       # Number of RADAR sweeps to aggregate per sample (passed to the loader).
lidar_nsweeps: 1                 # Number of LiDAR sweeps to aggregate per sample (used when use_lidar=true).
grid_dim: [200, 8, 200]          # Voxel grid (Z, Y, X). Y is vertical bins; BEV plane is Z (forward) × X (right).

# ======================
# Core task selection
# ======================
train_task: 'both'               # 'object' (vehicle layer only), 'map' (semantic map only), or 'both' (shared decoder).
use_obj_layer_only_on_map: true  # When 'both', map head predicts 7 classes and object head predicts the vehicle layer.

# ======================
# Image encoder & transformer lifting
# ======================
encoder_type: 'dino_v2'          # Image backbone: 'dino_v2' (ViT-B/14 adapter), 'vit_s', 'res50', or 'res101'.
use_pre_scaled_imgs: false       # If true, dataloader reads from custom_dataroot with pre-resized images.
init_query_with_image_feats: true
                                 # Initialize BEV queries by unprojecting multi-view image features into 3D, then BEV.
do_rgbcompress: true             # Minor convolutional compression step on 2D features (used in some paths).
do_shuffle_cams: true            # Randomize camera order within a sample (prevents positional bias).
use_multi_scale_img_feats: true  # Use 4/8/16/32 features as keys for cross-attention (richer, heavier).
num_layers: 6                    # Number of transformer encoder & fuser blocks (stacked).

# ======================
# RADAR branch
# ======================
radar_encoder_type: 'voxel_net'  # RADAR BEV encoder: currently 'voxel_net' (sparse voxel feature net).
use_rpn_radar: false             # If true, enables a small RPN head in VoxelNet (often didn’t help in your experiments).
use_radar: true                  # Enable RADAR branch at all.
use_radar_filters: true          # Apply classic radar filtering in the loader (e.g., remove outliers/ghosts).
use_radar_encoder: true          # If true, expect (features, coords, num_voxels) tuple and run VoxelNet; else occupancy.
use_radar_occupancy_map: false   # If true (and encoder supports), feed a 1-ch occupancy to the encoder instead of meta.
use_metaradar: false             # Enable 16-ch handcrafted meta features per radar point (when not using VoxelNet tuple).
use_shallow_metadata: true       # Enable 4-ch lightweight radar meta features (only used in non-voxelnet paths).

# ======================
# LiDAR branch
# ======================
lidar_encoder_type: 'voxel_net'  # LiDAR BEV encoder type when use_lidar_encoder=true.
use_lidar: true                  # Toggle LiDAR branch. (Your current data pipeline expects LiDAR present.)
use_lidar_encoder: true          # If false, use the lightweight “occupancy compressor” (Option A) we added.
use_lidar_occupancy_map: false   # If true (and encoder supports), provide a 1-ch LiDAR occupancy map to the encoder.

# ======================
# Misc / model plumbing
# ======================
device_ids: [0]                  # GPU ids used by torch.nn.DataParallel. batch_size must be divisible by len(device_ids).
freeze_dino: true                # Freeze DINO-V2 backbone (saves memory & stabilizes; unfreeze to fine-tune at a cost).
do_feat_enc_dec: true            # Apply BEV feature encoder–decoder refinement (ResNet18-like with skips) after fusion.
combine_feat_init_w_learned_q: true
                                 # Sum image-initialized queries with learned BEV queries + positional encodings.
model_type: 'transformer'        # Model family: 'transformer' (TransFusion-like lifting/fusion), others are legacy.
learnable_fuse_query: true       # Add a learned query tensor in the fuser stage (helps stabilize radar/lidar fusion).

# ======================
# Weights & Biases (W&B) logging
# ======================
group: 'BEV_CLR_AD'              # W&B group (so multiple runs show together).
notes: 'BEV_CLR_AD model: radar encoding, LiDAR encoding, DINO as image backbone, fusion and lifting based on transformer,
6 blocks, radar init, image init, multiscale feats, enc-dec, radar and LiDAR as query for fusion,
with learnable queries'
                                 # Free-form run notes shown in W&B UI.
name: 'BEVCar'                   # W&B run name (shown in the project dashboard).